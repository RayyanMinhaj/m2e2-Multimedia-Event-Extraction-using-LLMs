{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8dbbe6",
   "metadata": {},
   "source": [
    "# Reproducing M2E2 using a prompt-based LLM\n",
    "DFKI | Rayyan M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2736b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe27dba",
   "metadata": {},
   "source": [
    "### TO-DO\n",
    "- Individual JSON files for text-only, image-only, and separate for multimedia\n",
    "- For image-only, make sure the text for the image is captured through the captions.json\n",
    "- Fix the example formats (look at annotations.zip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e4b69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d58748a",
   "metadata": {},
   "source": [
    "### Defining schema/ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b573b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event obj : argument roles from M2E2 dataset paper\n",
    "\n",
    "event_arg = {\n",
    "    \"life.die\":[\"agent\", \"victim\", \"instrument\", \"place\"],\n",
    "    \"movement.transport\":[\"destination\", \"origin\", \"instrument\", \"agent\", \"artifact/person\"],\n",
    "    \"transaction.transfermoney\":[\"giver\",\"recipient\", \"money\"],\n",
    "    \"conflict.attack\":[\"attacker\", \"instrument\", \"place\", \"target\"],\n",
    "    \"conflict.demonstrate\":[\"demonstrator\", \"instrument\", \"police\", \"place\"],\n",
    "    \"contact.meet\":[\"participant\", \"place\"],\n",
    "    \"contact.phone-write\":[\"participant\", \"instrument\", \"place\"],\n",
    "    \"justice.arrestjail\":[\"agent\", \"person\", \"instrument\", \"place\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b78097b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article text length: 1205 characters\n",
      "Base folder: c:\\Users\\rayya\\Desktop\\DFKI\\M2E2\\output\\VOA_EN_NW_2009.12.09.416313\n",
      "Article copied to: c:\\Users\\rayya\\Desktop\\DFKI\\M2E2\\output\\VOA_EN_NW_2009.12.09.416313\\articles\\VOA_EN_NW_2009.12.09.416313.rsd.txt\n",
      "Images copied: 3 into c:\\Users\\rayya\\Desktop\\DFKI\\M2E2\\output\\VOA_EN_NW_2009.12.09.416313\\images\n",
      "Empty JSON created at: c:\\Users\\rayya\\Desktop\\DFKI\\M2E2\\output\\VOA_EN_NW_2009.12.09.416313\\OUTPUT_VOA_EN_NW_2009.12.09.416313.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# 1) Set the article filename here (must exist under m2e2_rawdata/article)\n",
    "article_filename = \"VOA_EN_NW_2009.12.09.416313.rsd.txt\"  # change this\n",
    "\n",
    "# 2) Option: change matching behavior if needed\n",
    "strict_prefix_match = True  # if False, will also match images that merely contain the base name\n",
    "\n",
    "# --- Paths ---\n",
    "project_root = Path.cwd()\n",
    "article_dir = project_root / \"m2e2_rawdata\" / \"article\"\n",
    "images_dir = project_root / \"m2e2_rawdata\" / \"image\" / \"image\"\n",
    "\n",
    "# Validate paths\n",
    "article_path = article_dir / article_filename\n",
    "if not article_path.exists():\n",
    "    raise FileNotFoundError(f\"Article file not found: {article_path}\")\n",
    "if not images_dir.exists():\n",
    "    raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "\n",
    "# Read the article text into a variable\n",
    "article_text = article_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# Derive a base name by stripping .txt and optional .rsd suffix\n",
    "base_name = article_filename\n",
    "if base_name.endswith(\".txt\"):\n",
    "    base_name = base_name[:-4]\n",
    "if base_name.endswith(\".rsd\"):\n",
    "    base_name = base_name[:-4]\n",
    "\n",
    "\n",
    "output_root = project_root / \"output\"\n",
    "base_output_dir = output_root / base_name\n",
    "articles_output_dir = base_output_dir / \"articles\"\n",
    "images_output_dir = base_output_dir / \"images\"\n",
    "articles_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "images_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy the article file into articles subfolder\n",
    "shutil.copy2(article_path, articles_output_dir / article_filename)\n",
    "\n",
    "# Find corresponding images\n",
    "matched_images = []\n",
    "for img_path in images_dir.glob(\"*.jpg\"):\n",
    "    name = img_path.name\n",
    "    if strict_prefix_match:\n",
    "        if name.startswith(base_name):\n",
    "            matched_images.append(img_path)\n",
    "    else:\n",
    "        if base_name in name:\n",
    "            matched_images.append(img_path)\n",
    "\n",
    "# Copy images into the images subfolder\n",
    "for img in matched_images:\n",
    "    shutil.copy2(img, images_output_dir / img.name)\n",
    "\n",
    "# Create an empty JSON file at the base folder level\n",
    "json_path = base_output_dir / f\"OUTPUT_{base_name}.json\"\n",
    "json_path.write_text(\"\", encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Article text length: {len(article_text)} characters\")\n",
    "print(f\"Base folder: {base_output_dir}\")\n",
    "print(f\"Article copied to: {articles_output_dir / article_filename}\")\n",
    "print(f\"Images copied: {len(matched_images)} into {images_output_dir}\")\n",
    "print(f\"Empty JSON created at: {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac5451",
   "metadata": {},
   "source": [
    "## (i) Text-only extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4da8cb",
   "metadata": {},
   "source": [
    "In WASE, each sentence is paired with the most relevant image (via embedding similarity). Right now we are choosing to not do that but in future we will: Either ask the LLM to select which image best matches a sentence (if you can pass both text + images together), or compute a similarity score (text embedding vs image embedding) and pick the closest image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a41f1",
   "metadata": {},
   "source": [
    "TO-DO: Pass images in input and ask prompt to select images that best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75db451",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_json = [\n",
    "    {\n",
    "        \"sentence_id\": 1,\n",
    "        \"text\": \"UNECA Director Says Dangers in Guinea are Serious\",\n",
    "        \"events\": [\n",
    "            {\n",
    "                \"event_type\": \"Conflict.Attack\",\n",
    "                \"modality\": \"text\",\n",
    "                \"trigger\": {\"text\": \"Dangers\", \"char_start\": 22, \"char_end\": 28},\n",
    "                \"arguments\": [\n",
    "                    {\"role\": \"Place\", \"text\": \"Guinea\", \"char_start\": 32, \"char_end\": 37}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"sentence_id\": 2,\n",
    "        \"text\": \"Bodies of people killed during a rally are seen at the capital's main mosque in Conakry, Guinea\",\n",
    "        \"events\": [\n",
    "            {\n",
    "                \"event_type\": \"Conflict.Attack\",\n",
    "                \"modality\": \"text\",\n",
    "                \"trigger\": {\"text\": \"killed\", \"char_start\": 20, \"char_end\": 26},\n",
    "                \"arguments\": [\n",
    "                    {\"role\": \"Victim\", \"text\": \"people\", \"char_start\": 10, \"char_end\": 16},\n",
    "                    {\"role\": \"Place\", \"text\": \"Conakry, Guinea\", \"char_start\": 82, \"char_end\": 96}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cca6f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"  # smaller version (270m) - performance was not good at all\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\")\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "you are an information extraction system. Extract events and arguments for each sentence from the following article: {article_text}.\n",
    "\n",
    "RULES:\n",
    "1. Only use the following event types and their argument roles: {event_arg}\n",
    "2. Output **must be a valid JSON array** only, nothing else. **Do not add markdown, backslashes, escape characters, or extra text**.\n",
    "3. Only use double quotation marks (\") for JSON strings.\n",
    "4. Each event must include:\n",
    "    a. \"sentence id\": an iterator over all the sentences\n",
    "    b. \"text\": the sentence itself\n",
    "    c. \"events\": these further contain the following:\n",
    "        a. \"event_type\": the event type string.\n",
    "        b. \"modality\": always text.\n",
    "        c. \"trigger\": the word(S) that signal the event, with \"text\", \"char_start\", \"char_end\".\n",
    "        d. \"arguments\": a list of objects, each with:\n",
    "            i. \"role\": role from the ontology.\n",
    "            ii. \"text\" the argument string.\n",
    "            iii. \"char_start\": start character index of the argument string.\n",
    "            iv. \"char_end\": end character index of the argument string.\n",
    "5. Offsets are character indices in the sentence (0-based, inclusive-exclusive).\n",
    "6. **Do not include explanations, notes, comments, or any text outside the JSON array.**\n",
    "7. The output must be fully parseable by `json.loads()` in Python.\n",
    "\n",
    "\n",
    "Here is an example output, your output should follow this exact JSON format, there can be more or less sentence id depending on the article: {example_json}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,  # adjust based on expected length\n",
    "    do_sample=False  # deterministic output\n",
    ")\n",
    "\n",
    "#generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "generated_ids = output_ids[0][inputs['input_ids'].shape[1]:]\n",
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1719ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[\\n  {\\'sentence_id\\': 1, \\'text\\': \\'UNECA Director Says Dangers in Guinea are Serious\\', \\'events\\': [{\\'event_type\\': \\'Conflict.Attack\\', \\'modality\\': \\'text\\', \\'trigger\\': {\\'text\\': \\'Dangers\\', \\'char_start\\': 22, \\'char_end\\': 28}, \\'arguments\\': [{\\'role\\': \\'Place\\', \\'text\\': \\'Guinea\\', \\'char_start\\': 32, \\'char_end\\': 37}]}]},\\n  {\\'sentence_id\\': 2, \\'text\\': \"Bodies of people killed during a rally are seen at the capital\\'s main mosque in Conakry, Guinea\", \\'events\\': [{\\'event_type\\': \\'Conflict.Attack\\', \\'modality\\': \\'text\\', \\'trigger\\': {\\'text\\': \\'killed\\', \\'char_start\\': 20, \\'char_end\\': 26}, \\'arguments\\': [{\\'role\\': \\'Victim\\', \\'text\\': \\'people\\', \\'char_start\\': 10, \\'char_end\\': 16}, {\\'role\\': \\'Place\\', \\'text\\': \\'Conakry, Guinea\\', \\'char_start\\': 82, \\'char_end\\': 96}]}]}\\n]\\n```\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe5774b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"sentence_id\": 1,\n",
      "    \"text\": \"UNECA Director Says Dangers in Guinea are Serious\",\n",
      "    \"events\": [\n",
      "      {\n",
      "        \"event_type\": \"Conflict.Attack\",\n",
      "        \"modality\": \"text\",\n",
      "        \"trigger\": {\n",
      "          \"text\": \"Dangers\",\n",
      "          \"char_start\": 22,\n",
      "          \"char_end\": 28\n",
      "        },\n",
      "        \"arguments\": [\n",
      "          {\n",
      "            \"role\": \"Place\",\n",
      "            \"text\": \"Guinea\",\n",
      "            \"char_start\": 32,\n",
      "            \"char_end\": 37\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"sentence_id\": 2,\n",
      "    \"text\": \"Bodies of people killed during a rally are seen at the capital's main mosque in Conakry, Guinea\",\n",
      "    \"events\": [\n",
      "      {\n",
      "        \"event_type\": \"Conflict.Attack\",\n",
      "        \"modality\": \"text\",\n",
      "        \"trigger\": {\n",
      "          \"text\": \"killed\",\n",
      "          \"char_start\": 20,\n",
      "          \"char_end\": 26\n",
      "        },\n",
      "        \"arguments\": [\n",
      "          {\n",
      "            \"role\": \"Victim\",\n",
      "            \"text\": \"people\",\n",
      "            \"char_start\": 10,\n",
      "            \"char_end\": 16\n",
      "          },\n",
      "          {\n",
      "            \"role\": \"Place\",\n",
      "            \"text\": \"Conakry, Guinea\",\n",
      "            \"char_start\": 82,\n",
      "            \"char_end\": 96\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "cleaned_text = generated_text.strip('```json\\n').strip('```\\n')\n",
    "\n",
    "python_obj = ast.literal_eval(cleaned_text)\n",
    "\n",
    "json_data = json.dumps(python_obj, indent=2)\n",
    "\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d45ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: c:\\Users\\rayya\\Desktop\\DFKI\\M2E2\\output\\VOA_EN_NW_2009.12.09.416313\\OUTPUT_VOA_EN_NW_2009.12.09.416313.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    out_path = json_path  # from earlier cell\n",
    "except NameError:\n",
    "    project_root = Path.cwd()\n",
    "    base_output_dir = project_root / \"output\" / base_name\n",
    "    base_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = base_output_dir / f\"OUTPUT_{base_name}.json\"\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "print(f\"Saved JSON to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf4d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fca2419",
   "metadata": {},
   "source": [
    "## (ii) Image-only extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6689b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_json_img = [\n",
    "  {\n",
    "    \"image_id\": \"Actual_Image_Name.jpg\",\n",
    "    \"events\": [\n",
    "      {\n",
    "        \"event_type\": \"Conflict.Attack\",\n",
    "        \"modality\": \"image\",\n",
    "        \"trigger\": {\"text\": \"attack\"}, \n",
    "        \"arguments\": [\n",
    "          {\n",
    "            \"role\": \"Attacker\",\n",
    "            \"text\": \"soldiers\",\n",
    "            \"bbox\": [0.12, 0.40, 0.45, 0.78]\n",
    "          },\n",
    "          {\n",
    "            \"role\": \"Victim\",\n",
    "            \"text\": \"protesters\",\n",
    "            \"bbox\": [0.50, 0.35, 0.80, 0.70]\n",
    "          },\n",
    "          {\n",
    "            \"role\": \"Place\",\n",
    "            \"text\": \"Conakry\",\n",
    "            \"bbox\": [0.05, 0.10, 0.30, 0.25]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390d5cddc28a40789adb7e95276e9739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cpu\", offload_folder=\"offload\")\n",
    "\n",
    "\n",
    "\n",
    "# Example: load one image\n",
    "img_path = images_output_dir / matched_images[1].name  # from earlier matching\n",
    "#img_path = \"output\\VOA_EN_NW_2009.12.09.416313\\images\\VOA_EN_NW_2009.12.09.416313_0.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "<start_of_image><end_of_image>\n",
    "\n",
    "You are an event extraction system. Analyze the image called {matched_images[1].name} and extract events.\n",
    "\n",
    "RULES:\n",
    "1. Only use the following event types and their roles: {event_arg}\n",
    "2. Output **must be a valid JSON array** only, nothing else. **Do not add markdown, backslashes, escape characters, or extra text**.\n",
    "3. Each event must include:\n",
    "   - \"event_type\": the event type.\n",
    "   - \"modality\": always \"image\".\n",
    "   - \"trigger\": a short word/phrase describing the main event (string only).\n",
    "   - \"arguments\": a list of objects, each with:\n",
    "       * \"role\": role name\n",
    "       * \"text\": short phrase for the entity (e.g., \"soldier\", \"protesters\", \"gun\", \"car\").\n",
    "       * \"bbox\": [x_min, y_min, x_max, y_max] with normalized coordinates between 0 and 1.\n",
    "4. If no clear event is present, return an empty list [].\n",
    "5. Do not add notes, markdown, or comments — only strict JSON.\n",
    "\n",
    "\n",
    "here is an example \n",
    "\n",
    "Here is an example output, your output should follow this exact JSON format: {example_json_img}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = processor(text=prompt, images=[image], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c150087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.special_tokens_map)\n",
    "print(processor.tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18605332",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = inputs[\"input_ids\"].shape[1]\n",
    "generated_text = processor.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c280f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n[\\n  {\\n    \"image_id\": \"VOA_EN_NW_2009.12.09.416313_1.jpg\",\\n    \"events\": [\\n      {\\n        \"event_type\": \"life.die\",\\n        \"modality\": \"image\",\\n        \"trigger\": \"death\",\\n        \"arguments\": [\\n          {\\n            \"role\": \"victim\",\\n            \"text\": \"corpses\",\\n            \"bbox\": [\\n              0.0,\\n              0.1,\\n              0.6,\\n              0.95\\n            ]\\n          },\\n          {\\n            \"role\": \"place\",\\n            \"text\": \"ground\",\\n            \"bbox\": [\\n              0.0,\\n              0.0,\\n              1.0,\\n              0.3\\n            ]\\n          }\\n        ]\\n      },\\n      {\\n        \"event_type\": \"conflict.demonstrate\",\\n        \"modality\": \"image\",\\n        \"trigger\": \"police\",\\n        \"arguments\": [\\n          {\\n            \"role\": \"police\",\\n            \"text\": \"police\",\\n            \"bbox\": [\\n              0.3,\\n              0.2,\\n              0.8,\\n              0.9\\n            ]\\n          }\\n        ]\\n      }\\n    ]\\n  }\\n]\\n```'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40755465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"image_id\": \"VOA_EN_NW_2009.12.09.416313_1.jpg\",\n",
      "    \"events\": [\n",
      "      {\n",
      "        \"event_type\": \"life.die\",\n",
      "        \"modality\": \"image\",\n",
      "        \"trigger\": \"death\",\n",
      "        \"arguments\": [\n",
      "          {\n",
      "            \"role\": \"victim\",\n",
      "            \"text\": \"corpses\",\n",
      "            \"bbox\": [\n",
      "              0.0,\n",
      "              0.1,\n",
      "              0.6,\n",
      "              0.95\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"role\": \"place\",\n",
      "            \"text\": \"ground\",\n",
      "            \"bbox\": [\n",
      "              0.0,\n",
      "              0.0,\n",
      "              1.0,\n",
      "              0.3\n",
      "            ]\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"event_type\": \"conflict.demonstrate\",\n",
      "        \"modality\": \"image\",\n",
      "        \"trigger\": \"police\",\n",
      "        \"arguments\": [\n",
      "          {\n",
      "            \"role\": \"police\",\n",
      "            \"text\": \"police\",\n",
      "            \"bbox\": [\n",
      "              0.3,\n",
      "              0.2,\n",
      "              0.8,\n",
      "              0.9\n",
      "            ]\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import json\n",
    "\n",
    "cleaned_text = generated_text.strip('```json\\n').strip('```\\n')\n",
    "\n",
    "python_obj = ast.literal_eval(cleaned_text)\n",
    "\n",
    "json_data = json.dumps(python_obj, indent=2)\n",
    "\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc2717",
   "metadata": {},
   "source": [
    "### The text here needs to be taken from the captions file, instead of asking LLM to generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd876105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended JSON to: c:\\Users\\rayya\\Desktop\\DFKI\\M2E2\\output\\VOA_EN_NW_2009.12.09.416313\\OUTPUT_VOA_EN_NW_2009.12.09.416313.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    out_path = json_path  # from earlier cell\n",
    "except NameError:\n",
    "    project_root = Path.cwd()\n",
    "    base_output_dir = project_root / \"output\" / base_name\n",
    "    base_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = base_output_dir / f\"OUTPUT_{base_name}.json\"\n",
    "\n",
    "if out_path.exists():\n",
    "    with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        existing_data = json.load(f)\n",
    "else:\n",
    "    existing_data = []\n",
    "\n",
    "new_data = json.loads(json_data)  # convert string to Python object\n",
    "if isinstance(new_data, list):\n",
    "    existing_data.extend(new_data)\n",
    "else:\n",
    "    existing_data.append(new_data)\n",
    "\n",
    "# Save back to file\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(existing_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Appended JSON to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc16c5",
   "metadata": {},
   "source": [
    "## (iii) Cross-media alignment Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af0a59",
   "metadata": {},
   "source": [
    "We need to pair the right image to the right sentence, which means not relying on event type alone (Conflict.Attack is too broad for ex). An article can mention multiple attacks, and we need to pair the correct image. For this we will add a second filter for similarity score i.e, both must be the same event type AND semantically close. We use a transformer for semantic matching between text and image descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cd28579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a7d2e8a10344b5ab04b54ce83fcba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rayya\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rayya\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0626ffc7884a421dbbab6856af6f34fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165f02b1ded94895885e6d9de2b01f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7189e7635eae487db5a2c184f825fa0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2312c0a19bcf434fa256c85a95dbe8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d329769132544a9846a33ff1ec94f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f7256823c94643b6b06391a4b5cb6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73ebf02cd654f1888c507dbaec5de39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d615c680f7614112b2853a02a7eaee74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4831bd681f6940bb95ff7dae3a36136f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08772a72d94e492ab21b6cef6a6adaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Going to use all-MiniLM-L6-v2 because its a small/fast snetence embedding model that maps text into dense vector. Then we can compute cosine similarity b/w 2 embeddings.\n",
    "#Ex - \"killed people Guinea\" (from text) and \"attack protesters Conakry\" (from image) -----> High similarity because both talk about violence in Guinea.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c83ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_similarity(text_event, image_event):\n",
    "\n",
    "    if text_event[\"event_type\"].lower() != image_event[\"event_type\"].lower():\n",
    "        return 0.0\n",
    "    \n",
    "    else:\n",
    "        text_summary = text_event[\"trigger\"][\"text\"] + \" \" + \" \".join(a[\"text\"] for a in text_event[\"arguments\"])\n",
    "        image_summary = image_event[\"trigger\"][\"text\"] + \" \" + \" \".join([a[\"text\"] for a in image_event[\"arguments\"]])\n",
    "\n",
    "        emb1 = embedder.encode(text_summary, convert_to_tensor=True)\n",
    "        emb2 = embedder.encode(image_summary, convert_to_tensor=True)\n",
    "        sim = util.pytorch_cos_sim(emb1, emb2).item()\n",
    "\n",
    "        return sim\n",
    "\n",
    "\n",
    "def align_events(text_events, image_events, threshold=0.0):\n",
    "    multimedia_events=[]\n",
    "\n",
    "    for txt_event in text_events:\n",
    "        for img_event in image_events:\n",
    "            score = comp_similarity(txt_event, img_event)\n",
    "\n",
    "            if score > threshold:\n",
    "                multimedia_events.append({\n",
    "                    \"event_type\": txt_event[\"event_type\"],\n",
    "                    \"modality\": \"multimedia\",\n",
    "                    \"similarity\": score,\n",
    "                    \"text_event\": txt_event,\n",
    "                    \"image_event\": img_event\n",
    "                })\n",
    "\n",
    "    \n",
    "    return multimedia_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3cce6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended 2 multimedia events to c:\\Users\\rayya\\Desktop\\DFKI\\M2E2\\output\\VOA_EN_NW_2009.12.09.416313\\OUTPUT_VOA_EN_NW_2009.12.09.416313.json\n"
     ]
    }
   ],
   "source": [
    "project_root = Path.cwd()\n",
    "base_output_dir = project_root / \"output\" / \"VOA_EN_NW_2009.12.09.416313\"\n",
    "json_path = base_output_dir / \"OUTPUT_VOA_EN_NW_2009.12.09.416313.json\"\n",
    "\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "\n",
    "text_events = []\n",
    "image_events = []\n",
    "\n",
    "for entry in data:\n",
    "    if \"sentence_id\" in entry:  # text block\n",
    "        for ev in entry.get(\"events\", []):\n",
    "            if ev.get(\"modality\") == \"text\":\n",
    "                text_events.append(ev)\n",
    "    \n",
    "    elif \"image_id\" in entry:  # image block\n",
    "        for ev in entry.get(\"events\", []):\n",
    "            if ev.get(\"modality\") == \"image\":\n",
    "                image_events.append(ev)\n",
    "\n",
    "\n",
    "multimedia_events = align_events(text_events, image_events)\n",
    "\n",
    "\n",
    "data.extend(multimedia_events)\n",
    "\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Appended {len(multimedia_events)} multimedia events to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236cc54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
